{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0422e1f1-45d1-4891-ace4-c0f21d3ab9f2",
   "metadata": {},
   "source": [
    "![\"Persistent Logo\"](https://raw.githubusercontent.com/dattarajrao/ml_workshop/main/logo.png)\n",
    "# Fine-tuning Apple MLX framework \n",
    "### Instruction tune a 4B parameter Phi3 LLM, on SQL query builder dataset on HugingFace\n",
    "\n",
    "Author: Dattaraj Rao - https://www.linkedin.com/in/dattarajrao/ <br/>\n",
    "Model published at: https://huggingface.co/dattaraj/phi3-finetuned-mentalhealth <br/>\n",
    "Dataset used: https://huggingface.co/datasets/b-mc2/sql-create-context <br/>\n",
    "Reference: https://heidloff.net/article/apple-mlx-fine-tuning/\n",
    "\n",
    "Objective: Leverage a compilation of sql query creations compiled from real cases. Use this dataset to fine tune a 4B parameter language model - Phi3 - usng Apple MLX framework. Evaluate if using the dataset responses are more better quality SQL generations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3674cb5-f900-43c2-a225-c6fb81418508",
   "metadata": {},
   "source": [
    "### Check GPU availability on Apple Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f7315f5-db1d-46c3-8958-5684639d26fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68529cf9-a41c-4490-a89f-d5a354f1daca",
   "metadata": {},
   "source": [
    "### Compile data from HF Dataset\n",
    "\n",
    "Download CSV from: https://huggingface.co/datasets/b-mc2/sql-create-context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9165a02b-9968-4f77-99ef-6fcf99a8fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb27219f4104a45b4086b660579d764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_16 (lane VARCHAR, heat INTEGER)', 'role': 'system'}, {'content': 'What is the total lane for a heat larger than 7?', 'role': 'user'}, {'content': 'SELECT COUNT(lane) FROM table_name_16 WHERE heat > 7', 'role': 'assistant'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda64e9740094ce8a67d19c07e5a96f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633a6b8a1b3246928b70b60fb21370cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd44752297ad4f219736508ea80e8877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11884"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "folder=\"my-data-chat/\" \n",
    "\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(150))\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "dataset = dataset.train_test_split(test_size=50/150)\n",
    "dataset_test_valid = dataset['test'].train_test_split(0.5)\n",
    "print(dataset[\"train\"][45][\"messages\"])\n",
    "dataset[\"train\"].to_json(folder + \"train.jsonl\", orient=\"records\")\n",
    "\n",
    "dataset_test_valid[\"train\"].to_json(folder + \"test.jsonl\", orient=\"records\")\n",
    "dataset_test_valid[\"test\"].to_json(folder + \"valid.jsonl\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a82c266-1e77-4668-bf97-5d72cdac124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "folder_input='./my-data-chat/'\n",
    "folder_output='./my-data-text/'\n",
    "names=['test', 'train', 'valid']\n",
    "for name in names:\n",
    "    with open(folder_input + name + '.jsonl', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    with open(folder_output + name + '.jsonl', 'w') as outfile:\n",
    "        for json_str in json_list:\n",
    "            result = json.loads(json_str)\n",
    "            message = result['messages']\n",
    "            entry = {\n",
    "                \"text\": message[0]['content'] + '\\nUser:' + message[1]['content'] + '\\nAssistant:' + message[2]['content']\n",
    "            }\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9501b31-a9d4-45d8-bd39-4738cfe259c4",
   "metadata": {},
   "source": [
    "### Run MX framework inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eeef16f2-fd95-4e05-95f1-fd460e661630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e095f4e2fe7419484ca435166db6bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "prompt = \"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_71 (tie_no VARCHAR, away_team VARCHAR)\\nUser:What is Tie No, when Away Team is \\\"Millwall\\\"?\\nAssistant:SELECT tie_no FROM table_name_71 WHERE away_team = \\\"millwall\\\"\"\n",
    "model, tokenizer = load(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfdeee4-0927-4773-bfbb-0e9999e8b653",
   "metadata": {},
   "source": [
    "### Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf5e6944-e3a3-4464-8ce9-e93cb2856e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.24): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.25): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.26): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.27): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.28): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.29): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.30): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.31): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (norm): RMSNorm(3072, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(input_dims=3072, output_dims=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc353f49-7728-45dc-b31a-fe4eb5da3e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_71 (tie_no VARCHAR, away_team VARCHAR)\n",
      "User:What is Tie No, when Away Team is \"Millwall\"?\n",
      "Assistant:SELECT tie_no FROM table_name_71 WHERE away_team = \"millwall\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are an advanced SQL query translator. Users will ask you complex questions in English, and you will generate a SQL query based on the provided SCHEMA. The questions may involve multiple tables, conditions, and specific data retrieval.\n",
      "\n",
      "SCHEMA:\n",
      "CREATE TABLE team_info (team_id VARCHAR, team_name VARCHAR, country VARCHAR, founded_year INT)\n",
      "CREATE TABLE match_results (match_id VARCHAR, home_team VARCHAR,\n",
      "==========\n",
      "Prompt: 4.456 tokens-per-sec\n",
      "Generation: 14.924 tokens-per-sec\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are an advanced SQL query translator. Users will ask you complex questions in English, and you will generate a SQL query based on the provided SCHEMA. The questions may involve multiple tables, conditions, and specific data retrieval.\n",
      "\n",
      "SCHEMA:\n",
      "CREATE TABLE team_info (team_id VARCHAR, team_name VARCHAR, country VARCHAR, founded_year INT)\n",
      "CREATE TABLE match_results (match_id VARCHAR, home_team VARCHAR,\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe8d7a-0f65-448c-a65c-6d947283378e",
   "metadata": {},
   "source": [
    "## Fine tune with MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c300e6a6-0230-41f3-958d-dcc60225c5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 1 μs, total: 4 μs\n",
      "Wall time: 15 μs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 47290.50it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.041% (1.573M/3821.080M)\n",
      "Starting training..., iters: 1000\n",
      "Iter 1: Val loss 2.599, Val took 9.977s\n",
      "Iter 10: Train loss 2.295, Learning Rate 1.000e-05, It/sec 0.601, Tokens/sec 285.199, Trained Tokens 4745, Peak mem 9.629 GB\n",
      "Iter 20: Train loss 1.533, Learning Rate 1.000e-05, It/sec 0.703, Tokens/sec 299.395, Trained Tokens 9005, Peak mem 9.629 GB\n",
      "Iter 30: Train loss 1.052, Learning Rate 1.000e-05, It/sec 0.678, Tokens/sec 300.776, Trained Tokens 13440, Peak mem 9.629 GB\n",
      "Iter 40: Train loss 0.916, Learning Rate 1.000e-05, It/sec 0.683, Tokens/sec 300.041, Trained Tokens 17834, Peak mem 9.629 GB\n",
      "Iter 50: Train loss 0.830, Learning Rate 1.000e-05, It/sec 0.688, Tokens/sec 300.161, Trained Tokens 22194, Peak mem 9.629 GB\n",
      "Iter 60: Train loss 0.721, Learning Rate 1.000e-05, It/sec 0.723, Tokens/sec 315.488, Trained Tokens 26558, Peak mem 9.629 GB\n",
      "Iter 70: Train loss 0.775, Learning Rate 1.000e-05, It/sec 0.680, Tokens/sec 302.190, Trained Tokens 30999, Peak mem 9.629 GB\n",
      "Iter 80: Train loss 0.762, Learning Rate 1.000e-05, It/sec 0.664, Tokens/sec 303.991, Trained Tokens 35575, Peak mem 9.629 GB\n",
      "Iter 90: Train loss 0.751, Learning Rate 1.000e-05, It/sec 0.689, Tokens/sec 314.984, Trained Tokens 40144, Peak mem 9.629 GB\n",
      "Iter 100: Train loss 0.681, Learning Rate 1.000e-05, It/sec 0.725, Tokens/sec 307.632, Trained Tokens 44388, Peak mem 9.629 GB\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.643, Learning Rate 1.000e-05, It/sec 0.691, Tokens/sec 306.451, Trained Tokens 48821, Peak mem 9.629 GB\n",
      "Iter 120: Train loss 0.744, Learning Rate 1.000e-05, It/sec 0.686, Tokens/sec 311.892, Trained Tokens 53367, Peak mem 9.629 GB\n",
      "Iter 130: Train loss 0.648, Learning Rate 1.000e-05, It/sec 0.678, Tokens/sec 288.214, Trained Tokens 57617, Peak mem 9.629 GB\n",
      "Iter 140: Train loss 0.635, Learning Rate 1.000e-05, It/sec 0.671, Tokens/sec 298.078, Trained Tokens 62060, Peak mem 9.629 GB\n",
      "Iter 150: Train loss 0.666, Learning Rate 1.000e-05, It/sec 0.692, Tokens/sec 312.828, Trained Tokens 66582, Peak mem 9.629 GB\n",
      "Iter 160: Train loss 0.576, Learning Rate 1.000e-05, It/sec 0.761, Tokens/sec 311.912, Trained Tokens 70682, Peak mem 9.629 GB\n",
      "Iter 170: Train loss 0.647, Learning Rate 1.000e-05, It/sec 0.648, Tokens/sec 307.109, Trained Tokens 75419, Peak mem 9.629 GB\n",
      "Iter 180: Train loss 0.599, Learning Rate 1.000e-05, It/sec 0.710, Tokens/sec 306.564, Trained Tokens 79737, Peak mem 9.629 GB\n",
      "Iter 190: Train loss 0.624, Learning Rate 1.000e-05, It/sec 0.655, Tokens/sec 295.133, Trained Tokens 84244, Peak mem 9.629 GB\n",
      "Iter 200: Val loss 0.756, Val took 5.491s\n",
      "Iter 200: Train loss 0.575, Learning Rate 1.000e-05, It/sec 5.131, Tokens/sec 2325.178, Trained Tokens 88776, Peak mem 9.629 GB\n",
      "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.520, Learning Rate 1.000e-05, It/sec 0.705, Tokens/sec 308.834, Trained Tokens 93158, Peak mem 9.629 GB\n",
      "Iter 220: Train loss 0.535, Learning Rate 1.000e-05, It/sec 0.724, Tokens/sec 306.227, Trained Tokens 97389, Peak mem 9.629 GB\n",
      "Iter 230: Train loss 0.573, Learning Rate 1.000e-05, It/sec 0.672, Tokens/sec 306.567, Trained Tokens 101953, Peak mem 9.629 GB\n",
      "Iter 240: Train loss 0.510, Learning Rate 1.000e-05, It/sec 0.725, Tokens/sec 307.311, Trained Tokens 106190, Peak mem 9.629 GB\n",
      "Iter 250: Train loss 0.574, Learning Rate 1.000e-05, It/sec 0.585, Tokens/sec 279.777, Trained Tokens 110970, Peak mem 9.629 GB\n",
      "Iter 260: Train loss 0.477, Learning Rate 1.000e-05, It/sec 0.698, Tokens/sec 297.425, Trained Tokens 115230, Peak mem 9.629 GB\n",
      "Iter 270: Train loss 0.501, Learning Rate 1.000e-05, It/sec 0.657, Tokens/sec 300.828, Trained Tokens 119808, Peak mem 9.629 GB\n",
      "Iter 280: Train loss 0.477, Learning Rate 1.000e-05, It/sec 0.708, Tokens/sec 304.883, Trained Tokens 124112, Peak mem 9.629 GB\n",
      "Iter 290: Train loss 0.458, Learning Rate 1.000e-05, It/sec 0.649, Tokens/sec 300.944, Trained Tokens 128751, Peak mem 9.629 GB\n",
      "Iter 300: Train loss 0.499, Learning Rate 1.000e-05, It/sec 0.655, Tokens/sec 289.161, Trained Tokens 133164, Peak mem 9.629 GB\n",
      "Iter 300: Saved adapter weights to adapters/adapters.safetensors and adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.373, Learning Rate 1.000e-05, It/sec 0.736, Tokens/sec 305.421, Trained Tokens 137315, Peak mem 9.629 GB\n",
      "Iter 320: Train loss 0.495, Learning Rate 1.000e-05, It/sec 0.582, Tokens/sec 277.884, Trained Tokens 142086, Peak mem 9.629 GB\n",
      "Iter 330: Train loss 0.459, Learning Rate 1.000e-05, It/sec 0.667, Tokens/sec 300.745, Trained Tokens 146596, Peak mem 9.629 GB\n",
      "Iter 340: Train loss 0.387, Learning Rate 1.000e-05, It/sec 0.679, Tokens/sec 284.515, Trained Tokens 150787, Peak mem 9.629 GB\n",
      "Iter 350: Train loss 0.392, Learning Rate 1.000e-05, It/sec 0.658, Tokens/sec 300.674, Trained Tokens 155358, Peak mem 9.629 GB\n",
      "Iter 360: Train loss 0.384, Learning Rate 1.000e-05, It/sec 0.645, Tokens/sec 290.569, Trained Tokens 159866, Peak mem 9.629 GB\n",
      "Iter 370: Train loss 0.386, Learning Rate 1.000e-05, It/sec 0.663, Tokens/sec 299.473, Trained Tokens 164384, Peak mem 9.629 GB\n",
      "Iter 380: Train loss 0.373, Learning Rate 1.000e-05, It/sec 0.689, Tokens/sec 301.410, Trained Tokens 168758, Peak mem 9.629 GB\n",
      "Iter 390: Train loss 0.297, Learning Rate 1.000e-05, It/sec 0.733, Tokens/sec 310.620, Trained Tokens 172994, Peak mem 9.629 GB\n",
      "Iter 400: Val loss 0.946, Val took 5.439s\n",
      "Iter 400: Train loss 0.364, Learning Rate 1.000e-05, It/sec 7.062, Tokens/sec 3218.885, Trained Tokens 177552, Peak mem 9.629 GB\n",
      "Iter 400: Saved adapter weights to adapters/adapters.safetensors and adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.289, Learning Rate 1.000e-05, It/sec 0.583, Tokens/sec 256.132, Trained Tokens 181949, Peak mem 9.629 GB\n",
      "Iter 420: Train loss 0.345, Learning Rate 1.000e-05, It/sec 0.600, Tokens/sec 273.709, Trained Tokens 186512, Peak mem 9.629 GB\n",
      "Iter 430: Train loss 0.352, Learning Rate 1.000e-05, It/sec 0.676, Tokens/sec 307.610, Trained Tokens 191062, Peak mem 9.629 GB\n",
      "Iter 440: Train loss 0.270, Learning Rate 1.000e-05, It/sec 0.636, Tokens/sec 270.083, Trained Tokens 195306, Peak mem 9.629 GB\n",
      "Iter 450: Train loss 0.289, Learning Rate 1.000e-05, It/sec 0.699, Tokens/sec 310.317, Trained Tokens 199746, Peak mem 9.629 GB\n",
      "Iter 460: Train loss 0.273, Learning Rate 1.000e-05, It/sec 0.639, Tokens/sec 288.900, Trained Tokens 204269, Peak mem 9.629 GB\n",
      "Iter 470: Train loss 0.291, Learning Rate 1.000e-05, It/sec 0.653, Tokens/sec 290.805, Trained Tokens 208722, Peak mem 9.629 GB\n",
      "Iter 480: Train loss 0.234, Learning Rate 1.000e-05, It/sec 0.673, Tokens/sec 301.220, Trained Tokens 213199, Peak mem 9.629 GB\n",
      "Iter 490: Train loss 0.291, Learning Rate 1.000e-05, It/sec 0.584, Tokens/sec 275.554, Trained Tokens 217914, Peak mem 9.629 GB\n",
      "Iter 500: Train loss 0.221, Learning Rate 1.000e-05, It/sec 0.692, Tokens/sec 278.785, Trained Tokens 221940, Peak mem 9.629 GB\n",
      "Iter 500: Saved adapter weights to adapters/adapters.safetensors and adapters/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 0.221, Learning Rate 1.000e-05, It/sec 0.739, Tokens/sec 316.169, Trained Tokens 226217, Peak mem 9.629 GB\n",
      "Iter 520: Train loss 0.258, Learning Rate 1.000e-05, It/sec 0.567, Tokens/sec 269.276, Trained Tokens 230969, Peak mem 9.629 GB\n",
      "Iter 530: Train loss 0.172, Learning Rate 1.000e-05, It/sec 0.712, Tokens/sec 305.959, Trained Tokens 235267, Peak mem 9.629 GB\n",
      "Iter 540: Train loss 0.240, Learning Rate 1.000e-05, It/sec 0.628, Tokens/sec 297.154, Trained Tokens 240002, Peak mem 9.629 GB\n",
      "Iter 550: Train loss 0.197, Learning Rate 1.000e-05, It/sec 0.750, Tokens/sec 309.889, Trained Tokens 244134, Peak mem 9.629 GB\n",
      "Iter 560: Train loss 0.159, Learning Rate 1.000e-05, It/sec 0.755, Tokens/sec 315.060, Trained Tokens 248309, Peak mem 9.629 GB\n",
      "Iter 570: Train loss 0.178, Learning Rate 1.000e-05, It/sec 0.706, Tokens/sec 303.384, Trained Tokens 252605, Peak mem 9.629 GB\n",
      "Iter 580: Train loss 0.205, Learning Rate 1.000e-05, It/sec 0.627, Tokens/sec 303.059, Trained Tokens 257435, Peak mem 9.629 GB\n",
      "Iter 590: Train loss 0.181, Learning Rate 1.000e-05, It/sec 0.609, Tokens/sec 279.839, Trained Tokens 262030, Peak mem 9.629 GB\n",
      "Iter 600: Val loss 1.221, Val took 5.489s\n",
      "Iter 600: Train loss 0.172, Learning Rate 1.000e-05, It/sec 8.021, Tokens/sec 3447.571, Trained Tokens 266328, Peak mem 9.629 GB\n",
      "Iter 600: Saved adapter weights to adapters/adapters.safetensors and adapters/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 0.123, Learning Rate 1.000e-05, It/sec 0.733, Tokens/sec 304.925, Trained Tokens 270490, Peak mem 9.629 GB\n",
      "Iter 620: Train loss 0.199, Learning Rate 1.000e-05, It/sec 0.600, Tokens/sec 285.786, Trained Tokens 275252, Peak mem 9.629 GB\n",
      "Iter 630: Train loss 0.145, Learning Rate 1.000e-05, It/sec 0.704, Tokens/sec 310.658, Trained Tokens 279667, Peak mem 9.629 GB\n",
      "Iter 640: Train loss 0.143, Learning Rate 1.000e-05, It/sec 0.685, Tokens/sec 303.936, Trained Tokens 284105, Peak mem 9.629 GB\n",
      "Iter 650: Train loss 0.138, Learning Rate 1.000e-05, It/sec 0.663, Tokens/sec 292.943, Trained Tokens 288522, Peak mem 9.629 GB\n",
      "Iter 660: Train loss 0.113, Learning Rate 1.000e-05, It/sec 0.714, Tokens/sec 306.644, Trained Tokens 292817, Peak mem 9.629 GB\n",
      "Iter 670: Train loss 0.146, Learning Rate 1.000e-05, It/sec 0.633, Tokens/sec 299.701, Trained Tokens 297549, Peak mem 9.629 GB\n",
      "Iter 680: Train loss 0.136, Learning Rate 1.000e-05, It/sec 0.688, Tokens/sec 302.971, Trained Tokens 301954, Peak mem 9.629 GB\n",
      "Iter 690: Train loss 0.113, Learning Rate 1.000e-05, It/sec 0.682, Tokens/sec 309.410, Trained Tokens 306494, Peak mem 9.629 GB\n",
      "Iter 700: Train loss 0.114, Learning Rate 1.000e-05, It/sec 0.728, Tokens/sec 307.322, Trained Tokens 310716, Peak mem 9.629 GB\n",
      "Iter 700: Saved adapter weights to adapters/adapters.safetensors and adapters/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 0.121, Learning Rate 1.000e-05, It/sec 0.664, Tokens/sec 303.017, Trained Tokens 315280, Peak mem 9.629 GB\n",
      "Iter 720: Train loss 0.106, Learning Rate 1.000e-05, It/sec 0.726, Tokens/sec 302.962, Trained Tokens 319454, Peak mem 9.629 GB\n",
      "Iter 730: Train loss 0.098, Learning Rate 1.000e-05, It/sec 0.654, Tokens/sec 298.952, Trained Tokens 324026, Peak mem 9.629 GB\n",
      "Iter 740: Train loss 0.117, Learning Rate 1.000e-05, It/sec 0.657, Tokens/sec 302.712, Trained Tokens 328633, Peak mem 9.629 GB\n",
      "Iter 750: Train loss 0.102, Learning Rate 1.000e-05, It/sec 0.724, Tokens/sec 309.841, Trained Tokens 332910, Peak mem 9.629 GB\n",
      "Iter 760: Train loss 0.090, Learning Rate 1.000e-05, It/sec 0.722, Tokens/sec 313.387, Trained Tokens 337253, Peak mem 9.629 GB\n",
      "Iter 770: Train loss 0.088, Learning Rate 1.000e-05, It/sec 0.685, Tokens/sec 302.766, Trained Tokens 341676, Peak mem 9.629 GB\n",
      "Iter 780: Train loss 0.104, Learning Rate 1.000e-05, It/sec 0.671, Tokens/sec 291.338, Trained Tokens 346021, Peak mem 9.629 GB\n",
      "Iter 790: Train loss 0.108, Learning Rate 1.000e-05, It/sec 0.479, Tokens/sec 234.982, Trained Tokens 350922, Peak mem 9.629 GB\n",
      "Iter 800: Val loss 1.375, Val took 5.616s\n",
      "Iter 800: Train loss 0.086, Learning Rate 1.000e-05, It/sec 6.893, Tokens/sec 2882.706, Trained Tokens 355104, Peak mem 9.629 GB\n",
      "Iter 800: Saved adapter weights to adapters/adapters.safetensors and adapters/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 0.099, Learning Rate 1.000e-05, It/sec 0.637, Tokens/sec 286.095, Trained Tokens 359597, Peak mem 9.629 GB\n",
      "Iter 820: Train loss 0.077, Learning Rate 1.000e-05, It/sec 0.702, Tokens/sec 299.861, Trained Tokens 363871, Peak mem 9.629 GB\n",
      "Iter 830: Train loss 0.074, Learning Rate 1.000e-05, It/sec 0.673, Tokens/sec 296.161, Trained Tokens 368274, Peak mem 9.629 GB\n",
      "Iter 840: Train loss 0.086, Learning Rate 1.000e-05, It/sec 0.601, Tokens/sec 283.570, Trained Tokens 372989, Peak mem 9.629 GB\n",
      "Iter 850: Train loss 0.088, Learning Rate 1.000e-05, It/sec 0.712, Tokens/sec 306.800, Trained Tokens 377298, Peak mem 9.629 GB\n",
      "Iter 860: Train loss 0.069, Learning Rate 1.000e-05, It/sec 0.678, Tokens/sec 300.368, Trained Tokens 381730, Peak mem 9.629 GB\n",
      "Iter 870: Train loss 0.073, Learning Rate 1.000e-05, It/sec 0.727, Tokens/sec 307.552, Trained Tokens 385958, Peak mem 9.629 GB\n",
      "Iter 880: Train loss 0.085, Learning Rate 1.000e-05, It/sec 0.615, Tokens/sec 287.241, Trained Tokens 390628, Peak mem 9.629 GB\n",
      "Iter 890: Train loss 0.071, Learning Rate 1.000e-05, It/sec 0.651, Tokens/sec 282.637, Trained Tokens 394970, Peak mem 9.629 GB\n",
      "Iter 900: Train loss 0.084, Learning Rate 1.000e-05, It/sec 0.650, Tokens/sec 293.806, Trained Tokens 399492, Peak mem 9.629 GB\n",
      "Iter 900: Saved adapter weights to adapters/adapters.safetensors and adapters/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 0.066, Learning Rate 1.000e-05, It/sec 0.708, Tokens/sec 308.646, Trained Tokens 403850, Peak mem 9.629 GB\n",
      "Iter 920: Train loss 0.080, Learning Rate 1.000e-05, It/sec 0.597, Tokens/sec 282.166, Trained Tokens 408574, Peak mem 9.629 GB\n",
      "Iter 930: Train loss 0.071, Learning Rate 1.000e-05, It/sec 0.740, Tokens/sec 308.475, Trained Tokens 412740, Peak mem 9.629 GB\n",
      "Iter 940: Train loss 0.068, Learning Rate 1.000e-05, It/sec 0.623, Tokens/sec 285.613, Trained Tokens 417321, Peak mem 9.629 GB\n",
      "Iter 950: Train loss 0.077, Learning Rate 1.000e-05, It/sec 0.679, Tokens/sec 296.183, Trained Tokens 421686, Peak mem 9.629 GB\n",
      "Iter 960: Train loss 0.062, Learning Rate 1.000e-05, It/sec 0.719, Tokens/sec 309.911, Trained Tokens 425997, Peak mem 9.629 GB\n",
      "Iter 970: Train loss 0.076, Learning Rate 1.000e-05, It/sec 0.685, Tokens/sec 306.623, Trained Tokens 430472, Peak mem 9.629 GB\n",
      "Iter 980: Train loss 0.058, Learning Rate 1.000e-05, It/sec 0.681, Tokens/sec 308.532, Trained Tokens 435005, Peak mem 9.629 GB\n",
      "Iter 990: Train loss 0.062, Learning Rate 1.000e-05, It/sec 0.720, Tokens/sec 312.218, Trained Tokens 439343, Peak mem 9.629 GB\n",
      "Iter 1000: Val loss 1.489, Val took 5.440s\n",
      "Iter 1000: Train loss 0.074, Learning Rate 1.000e-05, It/sec 7.038, Tokens/sec 3192.986, Trained Tokens 443880, Peak mem 9.629 GB\n",
      "Iter 1000: Saved adapter weights to adapters/adapters.safetensors and adapters/0001000_adapters.safetensors.\n",
      "Saved final adapter weights to adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "!python -m mlx_lm.lora --model microsoft/Phi-3-mini-4k-instruct --train --data ./my-data-text/ --iters 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f4265-c03d-43ed-b956-0a61237fb0da",
   "metadata": {},
   "source": [
    "### Inference with base mode and with adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59424dc-71a1-41ec-a00e-be65c423472e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c30626052748eaa7bbb273981d7fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_71 (tie_no VARCHAR, away_team VARCHAR)\n",
      "User:What is Tie No, when Away Team is \"Millwall\"?\n",
      "Assistant:SELECT tie_no FROM table_name_71 WHERE away_team = \"millwall\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are an advanced SQL query translator. Users will ask you complex questions in English, and you will generate a SQL query based on the provided SCHEMA. The questions may involve multiple tables, conditions, and specific data retrieval.\n",
      "\n",
      "SCHEMA:\n",
      "CREATE TABLE team_info (team_id VARCHAR, team_name VARCHAR, country VARCHAR, founded_year INT)\n",
      "CREATE TABLE match_results (match_id VARCHAR, home_team VARCHAR,\n",
      "==========\n",
      "Prompt: 118.658 tokens-per-sec\n",
      "Generation: 17.072 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a08766-8a16-4fc2-8301-2539d8ba8c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7253bf5c78ed43fea5df6ea31670bc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_71 (tie_no VARCHAR, away_team VARCHAR)\n",
      "User:What is Tie No, when Away Team is \"Millwall\"?\n",
      "Assistant:SELECT tie_no FROM table_name_71 WHERE away_team = \"millwall\"\n",
      "\n",
      "==========\n",
      "Prompt: 125.284 tokens-per-sec\n",
      "Generation: 0.000 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load(\"microsoft/Phi-3-mini-4k-instruct\", adapter_path=\"./adapters/\")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ff163-2393-430a-9b2d-300635611210",
   "metadata": {},
   "source": [
    "### Fuse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb46e865-e904-45fe-91b1-4d71531cdd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 40122.11it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.fuse --model microsoft/Phi-3-mini-4k-instruct --adapter-path adapters --save-path phi3-it-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99919f42-88e2-422f-b811-d16ea264bc1d",
   "metadata": {},
   "source": [
    "### Create GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e7666bd-a99f-4488-ad3c-40af50b9fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: phi3-it-sql\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 32000\n",
      "INFO:gguf.vocab:Setting special token type unk to 0\n",
      "INFO:gguf.vocab:Setting special token type pad to 32000\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {3072, 32064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> F16, shape = {3072, 32064}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:phi3-it-sql.gguf: n_tensors = 195, total_size = 7.6G\n",
      "Writing: 100%|███████████████████████████| 7.64G/7.64G [00:15<00:00, 483Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to phi3-it-sql.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py './phi3-it-sql' --outfile phi3-it-sql.gguf --outtype f16 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b16c1-547a-4132-82f4-1abe962f9399",
   "metadata": {},
   "source": [
    "## The GGUF file can now be published on HuggingFace Hub or used in libraries like Llama.CPP. \n",
    "\n",
    "# Happy fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6f6be-287a-4072-8afc-2d8db840ef97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
