{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0422e1f1-45d1-4891-ace4-c0f21d3ab9f2",
   "metadata": {},
   "source": [
    "![\"Persistent Logo\"](https://raw.githubusercontent.com/dattarajrao/ml_workshop/main/logo.png)\n",
    "# Fine-tuning Apple MLX framework \n",
    "### Instruction tune a 4B parameter LLM, on Mental Health dataset from Kaggle\n",
    "\n",
    "Author: Dattaraj Rao - https://www.linkedin.com/in/dattarajrao/ <br/>\n",
    "Model published at: https://huggingface.co/dattaraj/phi3-finetuned-mentalhealth\n",
    "Dataset used: https://www.kaggle.com/datasets/narendrageek/mental-health-faq-for-chatbot\n",
    "\n",
    "Objective: Leverage a compilation of mental health faq discussions compiled from real conversations. Use this dataset to fine tune a 4B parameter language model - Phi3 - usng Apple MLX framework. Evaluate if using the dataset resonses are more empathetic as compared to raw LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3674cb5-f900-43c2-a225-c6fb81418508",
   "metadata": {},
   "source": [
    "### Check GPU availability on Apple Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f7315f5-db1d-46c3-8958-5684639d26fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 8.00 MB, other allocations: 21.73 GB, max allowed: 20.40 GB). Tried to allocate 256 bytes on shared pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      3\u001b[0m     mps_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmps_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m (x)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 8.00 MB, other allocations: 21.73 GB, max allowed: 20.40 GB). Tried to allocate 256 bytes on shared pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68529cf9-a41c-4490-a89f-d5a354f1daca",
   "metadata": {},
   "source": [
    "### Compile data from CSV\n",
    "\n",
    "Download CSV from: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9165a02b-9968-4f77-99ef-6fcf99a8fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question_ID</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1590140</td>\n",
       "      <td>What does it mean to have a mental illness?</td>\n",
       "      <td>Mental illnesses are health conditions that di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2110618</td>\n",
       "      <td>Who does mental illness affect?</td>\n",
       "      <td>It is estimated that mental illness affects 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6361820</td>\n",
       "      <td>What causes mental illness?</td>\n",
       "      <td>It is estimated that mental illness affects 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9434130</td>\n",
       "      <td>What are some of the warning signs of mental i...</td>\n",
       "      <td>Symptoms of mental health disorders vary depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7657263</td>\n",
       "      <td>Can people with mental illness recover?</td>\n",
       "      <td>When healing from mental illness, early identi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question_ID                                          Questions  \\\n",
       "0      1590140        What does it mean to have a mental illness?   \n",
       "1      2110618                    Who does mental illness affect?   \n",
       "2      6361820                        What causes mental illness?   \n",
       "3      9434130  What are some of the warning signs of mental i...   \n",
       "4      7657263            Can people with mental illness recover?   \n",
       "\n",
       "                                             Answers  \n",
       "0  Mental illnesses are health conditions that di...  \n",
       "1  It is estimated that mental illness affects 1 ...  \n",
       "2  It is estimated that mental illness affects 1 ...  \n",
       "3  Symptoms of mental health disorders vary depen...  \n",
       "4  When healing from mental illness, early identi...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../Datasets/Mental_Health_FAQ.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e84b883-311e-4bd1-a904-f1a9fb482519",
   "metadata": {},
   "source": [
    "### Write traning and validation jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdbb27b6-b96b-459c-af9a-d2e3d096cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('synth-mentalhealth/train.jsonl', 'w') as tr_file, open('synth-mentalhealth/valid.jsonl', 'w') as vl_file:\n",
    "    tot_rows = len(df)\n",
    "    counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        row['Questions'] = row['Questions'].encode('ascii', 'ignore')\n",
    "        row['Answers'] = row['Answers'].encode('ascii', 'ignore')\n",
    "        counter = counter + 1\n",
    "        if counter < tot_rows - 25:\n",
    "            tr_file.write(json.dumps({\"text\": f\"<|user|>\\n{row['Questions']} <|end|>\\n<|assistant|>\\n{row['Answers']} <|end|>\"}) + \"\\n\") \n",
    "        else:\n",
    "            vl_file.write(json.dumps({\"text\": f\"<|user|>\\n{row['Questions']} <|end|>\\n<|assistant|> \\n{row['Answers']} <|end|>\"}) + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9501b31-a9d4-45d8-bd39-4738cfe259c4",
   "metadata": {},
   "source": [
    "### Run MX framework inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eeef16f2-fd95-4e05-95f1-fd460e661630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592c07469a50451f86a27730d9b47260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: What is the evidence on vaping?\n",
      "\n",
      "\n",
      "Vaping is a relatively new phenomenon, and there is still much to learn about its effects on health. However, some studies have suggested that vaping may have some benefits for smokers who want to quit or reduce their tobacco consumption. Here are some of the main findings from the research on vaping:\n",
      "\n",
      "- Vaping may help smokers quit or cut down on smoking. A systematic review of 38 studies found that e-cigarette use was associated with higher rates of abstinence from smoking than non-use or use of other nicotine replacement therapies (NRTs). The review also found that e-cigarette use was more effective than NRTs in helping smokers quit or reduce their smoking. However, the review also noted that the quality of the evidence was low to moderate, and that more research is needed to confirm these results and to understand the long-term effects of vaping.\n",
      "- Vaping may have fewer harmful effects than smoking. A review of 13 studies found that e-cigarette use was associated with lower levels of exposure to toxic and carcinogenic substances than smoking. The review also found that e-cigarette use was associated with lower levels of inflammation and oxidative stress, which are linked to various chronic diseases. However, the review also acknowledged that e-cigarettes are not risk-free, and that they may still pose some health risks, especially for long-term and heavy users.\n",
      "- Vaping may have different effects on different groups of smokers. A study of 1,000 smokers who used e-cigarettes found that the effects of vaping varied depending on the smokers' characteristics, such as their age, gender, motivation, and nicotine dependence. The study found that e-cigarette use was more effective in helping smokers quit or reduce their smoking among younger, female, motivated, and less dependent smokers. The study also found that e-cigarette use was associated with lower levels of nicotine dependence and withdrawal symptoms among these smokers. However, the study also found that e-cigarette use was less effective in helping smokers quit or reduce their smoking among older, male,\n",
      "==========\n",
      "Prompt: 7.739 tokens-per-sec\n",
      "Generation: 12.340 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=\"What is the evidence on vaping?\", verbose=True, max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe8d7a-0f65-448c-a65c-6d947283378e",
   "metadata": {},
   "source": [
    "## Fine tune with MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c300e6a6-0230-41f3-958d-dcc60225c5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 μs, sys: 16 μs, total: 23 μs\n",
      "Wall time: 281 μs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 13 files: 100%|████████████████████| 13/13 [00:00<00:00, 144248.55it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.041% (1.573M/3821.080M)\n",
      "Starting training..., iters: 1000\n",
      "Iter 1: Val loss 1.913, Val took 20.490s\n",
      "Iter 10: Train loss 2.184, Learning Rate 1.000e-05, It/sec 0.061, Tokens/sec 105.203, Trained Tokens 17139, Peak mem 40.406 GB\n",
      "Iter 20: Train loss 2.053, Learning Rate 1.000e-05, It/sec 0.157, Tokens/sec 215.326, Trained Tokens 30891, Peak mem 40.406 GB\n",
      "Iter 30: Train loss 1.971, Learning Rate 1.000e-05, It/sec 0.058, Tokens/sec 108.112, Trained Tokens 49555, Peak mem 40.406 GB\n",
      "Iter 40: Train loss 1.807, Learning Rate 1.000e-05, It/sec 0.121, Tokens/sec 156.161, Trained Tokens 62419, Peak mem 40.406 GB\n",
      "Iter 50: Train loss 1.760, Learning Rate 1.000e-05, It/sec 0.074, Tokens/sec 114.696, Trained Tokens 78018, Peak mem 40.406 GB\n",
      "Iter 60: Train loss 1.697, Learning Rate 1.000e-05, It/sec 0.062, Tokens/sec 101.271, Trained Tokens 94239, Peak mem 40.406 GB\n",
      "Iter 70: Train loss 1.622, Learning Rate 1.000e-05, It/sec 0.129, Tokens/sec 186.172, Trained Tokens 108654, Peak mem 40.406 GB\n",
      "Iter 80: Train loss 1.647, Learning Rate 1.000e-05, It/sec 0.057, Tokens/sec 103.744, Trained Tokens 126790, Peak mem 40.406 GB\n",
      "Iter 90: Train loss 1.545, Learning Rate 1.000e-05, It/sec 0.204, Tokens/sec 216.963, Trained Tokens 137445, Peak mem 40.406 GB\n",
      "Iter 100: Train loss 1.539, Learning Rate 1.000e-05, It/sec 0.059, Tokens/sec 100.721, Trained Tokens 154520, Peak mem 40.406 GB\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 1.545, Learning Rate 1.000e-05, It/sec 0.184, Tokens/sec 226.413, Trained Tokens 166832, Peak mem 40.406 GB\n",
      "Iter 120: Train loss 1.597, Learning Rate 1.000e-05, It/sec 0.120, Tokens/sec 198.352, Trained Tokens 183388, Peak mem 40.406 GB\n",
      "Iter 130: Train loss 1.357, Learning Rate 1.000e-05, It/sec 0.076, Tokens/sec 88.890, Trained Tokens 195133, Peak mem 40.406 GB\n",
      "Iter 140: Train loss 1.558, Learning Rate 1.000e-05, It/sec 0.111, Tokens/sec 176.930, Trained Tokens 211086, Peak mem 40.406 GB\n",
      "Iter 150: Train loss 1.397, Learning Rate 1.000e-05, It/sec 0.071, Tokens/sec 105.063, Trained Tokens 225805, Peak mem 40.406 GB\n",
      "Iter 160: Train loss 1.434, Learning Rate 1.000e-05, It/sec 0.058, Tokens/sec 111.101, Trained Tokens 244797, Peak mem 40.406 GB\n",
      "Iter 170: Train loss 1.434, Learning Rate 1.000e-05, It/sec 0.072, Tokens/sec 101.464, Trained Tokens 258983, Peak mem 40.406 GB\n",
      "Iter 180: Train loss 1.356, Learning Rate 1.000e-05, It/sec 0.114, Tokens/sec 180.791, Trained Tokens 274890, Peak mem 40.406 GB\n",
      "Iter 190: Train loss 1.441, Learning Rate 1.000e-05, It/sec 0.054, Tokens/sec 101.371, Trained Tokens 293570, Peak mem 40.406 GB\n",
      "Iter 200: Val loss 1.691, Val took 19.336s\n",
      "Iter 200: Train loss 1.234, Learning Rate 1.000e-05, It/sec 1.743, Tokens/sec 1904.650, Trained Tokens 304496, Peak mem 40.406 GB\n",
      "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 1.412, Learning Rate 1.000e-05, It/sec 0.052, Tokens/sec 102.643, Trained Tokens 324231, Peak mem 40.406 GB\n",
      "Iter 220: Train loss 1.016, Learning Rate 1.000e-05, It/sec 0.326, Tokens/sec 251.896, Trained Tokens 331958, Peak mem 40.406 GB\n",
      "Iter 230: Train loss 1.332, Learning Rate 1.000e-05, It/sec 0.130, Tokens/sec 198.924, Trained Tokens 347232, Peak mem 40.406 GB\n",
      "Iter 240: Train loss 1.153, Learning Rate 1.000e-05, It/sec 0.060, Tokens/sec 102.674, Trained Tokens 364354, Peak mem 40.406 GB\n",
      "Iter 250: Train loss 1.304, Learning Rate 1.000e-05, It/sec 0.062, Tokens/sec 115.149, Trained Tokens 382838, Peak mem 40.406 GB\n",
      "Iter 260: Train loss 1.023, Learning Rate 1.000e-05, It/sec 0.231, Tokens/sec 206.669, Trained Tokens 391785, Peak mem 40.406 GB\n",
      "Iter 270: Train loss 1.294, Learning Rate 1.000e-05, It/sec 0.053, Tokens/sec 109.332, Trained Tokens 412335, Peak mem 40.406 GB\n",
      "Iter 280: Train loss 1.313, Learning Rate 1.000e-05, It/sec 0.059, Tokens/sec 117.234, Trained Tokens 432133, Peak mem 40.406 GB\n",
      "Iter 290: Train loss 0.971, Learning Rate 1.000e-05, It/sec 0.161, Tokens/sec 192.907, Trained Tokens 444149, Peak mem 40.406 GB\n",
      "Iter 300: Train loss 1.147, Learning Rate 1.000e-05, It/sec 0.059, Tokens/sec 112.489, Trained Tokens 463166, Peak mem 40.406 GB\n",
      "Iter 300: Saved adapter weights to adapters/adapters.safetensors and adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.780, Learning Rate 1.000e-05, It/sec 0.361, Tokens/sec 257.878, Trained Tokens 470309, Peak mem 40.406 GB\n",
      "Iter 320: Train loss 0.933, Learning Rate 1.000e-05, It/sec 0.154, Tokens/sec 200.388, Trained Tokens 483282, Peak mem 40.406 GB\n",
      "Iter 330: Train loss 1.131, Learning Rate 1.000e-05, It/sec 0.041, Tokens/sec 84.393, Trained Tokens 503957, Peak mem 40.406 GB\n",
      "Iter 340: Train loss 0.985, Learning Rate 1.000e-05, It/sec 0.125, Tokens/sec 181.025, Trained Tokens 518469, Peak mem 40.406 GB\n",
      "Iter 350: Train loss 0.853, Learning Rate 1.000e-05, It/sec 0.203, Tokens/sec 210.688, Trained Tokens 528850, Peak mem 40.406 GB\n",
      "Iter 360: Train loss 1.122, Learning Rate 1.000e-05, It/sec 0.050, Tokens/sec 104.464, Trained Tokens 549780, Peak mem 40.406 GB\n",
      "Iter 370: Train loss 0.899, Learning Rate 1.000e-05, It/sec 0.121, Tokens/sec 176.581, Trained Tokens 564344, Peak mem 40.406 GB\n",
      "Iter 380: Train loss 1.099, Learning Rate 1.000e-05, It/sec 0.042, Tokens/sec 88.883, Trained Tokens 585746, Peak mem 40.406 GB\n",
      "Iter 390: Train loss 0.723, Learning Rate 1.000e-05, It/sec 0.215, Tokens/sec 213.474, Trained Tokens 595680, Peak mem 40.406 GB\n",
      "Iter 400: Val loss 1.851, Val took 19.397s\n",
      "Iter 400: Train loss 0.945, Learning Rate 1.000e-05, It/sec 2.620, Tokens/sec 4566.425, Trained Tokens 613110, Peak mem 40.406 GB\n",
      "Iter 400: Saved adapter weights to adapters/adapters.safetensors and adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.861, Learning Rate 1.000e-05, It/sec 0.121, Tokens/sec 171.508, Trained Tokens 627281, Peak mem 40.406 GB\n",
      "Iter 420: Train loss 1.047, Learning Rate 1.000e-05, It/sec 0.060, Tokens/sec 116.442, Trained Tokens 646585, Peak mem 40.406 GB\n",
      "Iter 430: Train loss 0.555, Learning Rate 1.000e-05, It/sec 0.197, Tokens/sec 182.337, Trained Tokens 655829, Peak mem 40.406 GB\n",
      "Iter 440: Train loss 0.815, Learning Rate 1.000e-05, It/sec 0.165, Tokens/sec 204.294, Trained Tokens 668219, Peak mem 40.406 GB\n",
      "Iter 450: Train loss 0.927, Learning Rate 1.000e-05, It/sec 0.059, Tokens/sec 112.312, Trained Tokens 687225, Peak mem 40.406 GB\n",
      "Iter 460: Train loss 0.770, Learning Rate 1.000e-05, It/sec 0.160, Tokens/sec 191.225, Trained Tokens 699155, Peak mem 40.406 GB\n",
      "Iter 470: Train loss 0.717, Learning Rate 1.000e-05, It/sec 0.059, Tokens/sec 97.364, Trained Tokens 715702, Peak mem 40.406 GB\n",
      "Iter 480: Train loss 0.790, Learning Rate 1.000e-05, It/sec 0.065, Tokens/sec 107.437, Trained Tokens 732238, Peak mem 40.406 GB\n",
      "Iter 490: Train loss 1.031, Learning Rate 1.000e-05, It/sec 0.092, Tokens/sec 166.569, Trained Tokens 750350, Peak mem 40.406 GB\n",
      "Iter 500: Train loss 0.657, Learning Rate 1.000e-05, It/sec 0.063, Tokens/sec 99.574, Trained Tokens 766091, Peak mem 40.406 GB\n",
      "Iter 500: Saved adapter weights to adapters/adapters.safetensors and adapters/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 0.855, Learning Rate 1.000e-05, It/sec 0.067, Tokens/sec 112.493, Trained Tokens 782897, Peak mem 40.406 GB\n",
      "Iter 520: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.130, Tokens/sec 153.751, Trained Tokens 794746, Peak mem 40.406 GB\n",
      "Iter 530: Train loss 0.612, Learning Rate 1.000e-05, It/sec 0.133, Tokens/sec 164.268, Trained Tokens 807087, Peak mem 40.406 GB\n",
      "Iter 540: Train loss 0.820, Learning Rate 1.000e-05, It/sec 0.062, Tokens/sec 108.528, Trained Tokens 824670, Peak mem 40.406 GB\n",
      "Iter 550: Train loss 0.760, Learning Rate 1.000e-05, It/sec 0.112, Tokens/sec 168.559, Trained Tokens 839659, Peak mem 40.406 GB\n",
      "Iter 560: Train loss 0.573, Learning Rate 1.000e-05, It/sec 0.074, Tokens/sec 101.879, Trained Tokens 853435, Peak mem 40.406 GB\n",
      "Iter 570: Train loss 0.797, Learning Rate 1.000e-05, It/sec 0.105, Tokens/sec 169.409, Trained Tokens 869571, Peak mem 40.406 GB\n",
      "Iter 580: Train loss 0.649, Learning Rate 1.000e-05, It/sec 0.045, Tokens/sec 81.565, Trained Tokens 887883, Peak mem 40.406 GB\n",
      "Iter 590: Train loss 0.553, Learning Rate 1.000e-05, It/sec 0.135, Tokens/sec 175.311, Trained Tokens 900889, Peak mem 40.406 GB\n",
      "Iter 600: Val loss 1.977, Val took 19.353s\n",
      "Iter 600: Train loss 0.807, Learning Rate 1.000e-05, It/sec 2.174, Tokens/sec 4030.185, Trained Tokens 919428, Peak mem 40.406 GB\n",
      "Iter 600: Saved adapter weights to adapters/adapters.safetensors and adapters/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 0.710, Learning Rate 1.000e-05, It/sec 0.130, Tokens/sec 182.362, Trained Tokens 933423, Peak mem 40.406 GB\n",
      "Iter 620: Train loss 0.440, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 196.944, Trained Tokens 943980, Peak mem 40.406 GB\n",
      "Iter 630: Train loss 0.749, Learning Rate 1.000e-05, It/sec 0.050, Tokens/sec 91.227, Trained Tokens 962115, Peak mem 40.406 GB\n",
      "Iter 640: Train loss 0.496, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 204.843, Trained Tokens 972904, Peak mem 40.406 GB\n",
      "Iter 650: Train loss 0.753, Learning Rate 1.000e-05, It/sec 0.058, Tokens/sec 113.760, Trained Tokens 992464, Peak mem 40.406 GB\n",
      "Iter 660: Train loss 0.475, Learning Rate 1.000e-05, It/sec 0.150, Tokens/sec 169.128, Trained Tokens 1003766, Peak mem 40.406 GB\n",
      "Iter 670: Train loss 0.643, Learning Rate 1.000e-05, It/sec 0.059, Tokens/sec 103.405, Trained Tokens 1021416, Peak mem 40.406 GB\n",
      "Iter 680: Train loss 0.635, Learning Rate 1.000e-05, It/sec 0.071, Tokens/sec 115.380, Trained Tokens 1037664, Peak mem 40.406 GB\n",
      "Iter 690: Train loss 0.832, Learning Rate 1.000e-05, It/sec 0.053, Tokens/sec 120.813, Trained Tokens 1060330, Peak mem 40.406 GB\n",
      "Iter 700: Train loss 0.465, Learning Rate 1.000e-05, It/sec 0.216, Tokens/sec 236.770, Trained Tokens 1071275, Peak mem 40.406 GB\n",
      "Iter 700: Saved adapter weights to adapters/adapters.safetensors and adapters/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.068, Tokens/sec 113.560, Trained Tokens 1088063, Peak mem 40.406 GB\n",
      "Iter 720: Train loss 0.466, Learning Rate 1.000e-05, It/sec 0.189, Tokens/sec 217.468, Trained Tokens 1099560, Peak mem 40.406 GB\n",
      "Iter 730: Train loss 0.580, Learning Rate 1.000e-05, It/sec 0.121, Tokens/sec 182.539, Trained Tokens 1114645, Peak mem 40.406 GB\n",
      "Iter 740: Train loss 0.427, Learning Rate 1.000e-05, It/sec 0.078, Tokens/sec 102.565, Trained Tokens 1127845, Peak mem 40.406 GB\n",
      "Iter 750: Train loss 0.498, Learning Rate 1.000e-05, It/sec 0.068, Tokens/sec 113.577, Trained Tokens 1144521, Peak mem 40.406 GB\n",
      "Iter 760: Train loss 0.811, Learning Rate 1.000e-05, It/sec 0.084, Tokens/sec 160.600, Trained Tokens 1163734, Peak mem 40.406 GB\n",
      "Iter 770: Train loss 0.348, Learning Rate 1.000e-05, It/sec 0.250, Tokens/sec 242.346, Trained Tokens 1173419, Peak mem 40.406 GB\n",
      "Iter 780: Train loss 0.552, Learning Rate 1.000e-05, It/sec 0.064, Tokens/sec 106.270, Trained Tokens 1190141, Peak mem 40.406 GB\n",
      "Iter 790: Train loss 0.621, Learning Rate 1.000e-05, It/sec 0.065, Tokens/sec 112.057, Trained Tokens 1207447, Peak mem 40.406 GB\n",
      "Iter 800: Val loss 2.236, Val took 22.526s\n",
      "Iter 800: Train loss 0.410, Learning Rate 1.000e-05, It/sec 1.003, Tokens/sec 1299.260, Trained Tokens 1220397, Peak mem 40.406 GB\n",
      "Iter 800: Saved adapter weights to adapters/adapters.safetensors and adapters/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 0.535, Learning Rate 1.000e-05, It/sec 0.061, Tokens/sec 101.443, Trained Tokens 1237005, Peak mem 40.406 GB\n",
      "Iter 820: Train loss 0.631, Learning Rate 1.000e-05, It/sec 0.056, Tokens/sec 109.516, Trained Tokens 1256672, Peak mem 40.406 GB\n",
      "Iter 830: Train loss 0.273, Learning Rate 1.000e-05, It/sec 0.301, Tokens/sec 272.991, Trained Tokens 1265737, Peak mem 40.406 GB\n",
      "Iter 840: Train loss 0.630, Learning Rate 1.000e-05, It/sec 0.058, Tokens/sec 111.294, Trained Tokens 1285015, Peak mem 40.406 GB\n",
      "Iter 850: Train loss 0.465, Learning Rate 1.000e-05, It/sec 0.151, Tokens/sec 205.559, Trained Tokens 1298616, Peak mem 40.406 GB\n",
      "Iter 860: Train loss 0.387, Learning Rate 1.000e-05, It/sec 0.073, Tokens/sec 110.912, Trained Tokens 1313870, Peak mem 40.406 GB\n",
      "Iter 870: Train loss 0.480, Learning Rate 1.000e-05, It/sec 0.131, Tokens/sec 181.224, Trained Tokens 1327695, Peak mem 40.406 GB\n",
      "Iter 880: Train loss 0.493, Learning Rate 1.000e-05, It/sec 0.063, Tokens/sec 104.344, Trained Tokens 1344338, Peak mem 40.406 GB\n",
      "Iter 890: Train loss 0.530, Learning Rate 1.000e-05, It/sec 0.067, Tokens/sec 113.056, Trained Tokens 1361195, Peak mem 40.406 GB\n",
      "Iter 900: Train loss 0.380, Learning Rate 1.000e-05, It/sec 0.154, Tokens/sec 204.316, Trained Tokens 1374450, Peak mem 40.406 GB\n",
      "Iter 900: Saved adapter weights to adapters/adapters.safetensors and adapters/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 0.245, Learning Rate 1.000e-05, It/sec 0.231, Tokens/sec 210.949, Trained Tokens 1383565, Peak mem 40.406 GB\n",
      "Iter 920: Train loss 0.841, Learning Rate 1.000e-05, It/sec 0.036, Tokens/sec 97.925, Trained Tokens 1410416, Peak mem 40.406 GB\n",
      "Iter 930: Train loss 0.379, Learning Rate 1.000e-05, It/sec 0.135, Tokens/sec 178.368, Trained Tokens 1423593, Peak mem 40.406 GB\n",
      "Iter 940: Train loss 0.299, Learning Rate 1.000e-05, It/sec 0.230, Tokens/sec 264.395, Trained Tokens 1435086, Peak mem 40.406 GB\n",
      "Iter 950: Train loss 0.415, Learning Rate 1.000e-05, It/sec 0.066, Tokens/sec 99.920, Trained Tokens 1450144, Peak mem 40.406 GB\n",
      "Iter 960: Train loss 0.365, Learning Rate 1.000e-05, It/sec 0.160, Tokens/sec 220.190, Trained Tokens 1463910, Peak mem 40.406 GB\n",
      "Iter 970: Train loss 0.479, Learning Rate 1.000e-05, It/sec 0.062, Tokens/sec 109.724, Trained Tokens 1481582, Peak mem 40.406 GB\n",
      "Iter 980: Train loss 0.457, Learning Rate 1.000e-05, It/sec 0.068, Tokens/sec 116.008, Trained Tokens 1498540, Peak mem 40.406 GB\n",
      "Iter 990: Train loss 0.376, Learning Rate 1.000e-05, It/sec 0.138, Tokens/sec 183.914, Trained Tokens 1511895, Peak mem 40.406 GB\n",
      "Iter 1000: Val loss 2.394, Val took 19.335s\n",
      "Iter 1000: Train loss 0.306, Learning Rate 1.000e-05, It/sec 6.235, Tokens/sec 8023.404, Trained Tokens 1524763, Peak mem 40.406 GB\n",
      "Iter 1000: Saved adapter weights to adapters/adapters.safetensors and adapters/0001000_adapters.safetensors.\n",
      "Saved final adapter weights to adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "!python -m mlx_lm.lora --model microsoft/Phi-3-mini-4k-instruct --train --data ./synth-mentalhealth/ --iters 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f4265-c03d-43ed-b956-0a61237fb0da",
   "metadata": {},
   "source": [
    "### Inference with adapter\n",
    "\n",
    "Save ./adapters folder as ./lora-mental-health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96f15741-32e4-41f5-b45b-6fa1a06aface",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|████████████████████| 13/13 [00:00<00:00, 128902.96it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "==========\n",
      "Prompt: <|user|>\n",
      "How do I deal with a mental health patient who is violent?<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Mental illness can impair an individuals ability to think clearly, recognize reality, and make reason decisions. In some cases, mental illness may lead to behavior that can be harmful to themselves or others. When a person is in crisis, they may require immediate intervention or hospitalization to stabilize their symptoms. This is known as an involuntary commitment. \n",
      "\n",
      "Involuntary commitment is a legal procedure in which someone who is considered a danger to\n",
      "==========\n",
      "Prompt: 28.654 tokens-per-sec\n",
      "Generation: 17.097 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.generate --model microsoft/Phi-3-mini-4k-instruct --adapter-path ./lora-mental-health --prompt \"How do I deal with a mental health patient who is violent?\" --eos-token \" <|end|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffbac7-50e4-4b40-9585-574edbccc2d7",
   "metadata": {},
   "source": [
    "### Without adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fbc80cf-741d-48c5-8cbe-5f88f50f681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|████████████████████| 13/13 [00:00<00:00, 153162.79it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "==========\n",
      "Prompt: <|user|>\n",
      "How do I deal with a mental health patient who is violent?<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "As an AI, I must emphasize the importance of prioritizing safety and seeking professional guidance. Here are some general steps to approach a situation involving a violent mental health patient:\n",
      "\n",
      "1. Assess the situation: Remain calm and assess the level of threat. Ensure you and others around you are in a safe location.\n",
      "\n",
      "2. Call for help: If the situation is severe, call emergency services (911 in the United States) to report the incident\n",
      "==========\n",
      "Prompt: 30.723 tokens-per-sec\n",
      "Generation: 17.319 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.generate --model microsoft/Phi-3-mini-4k-instruct --prompt \"How do I deal with a mental health patient who is violent?\" --eos-token \"<|end|>\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ff163-2393-430a-9b2d-300635611210",
   "metadata": {},
   "source": [
    "### Fuse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb46e865-e904-45fe-91b1-4d71531cdd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 13 files: 100%|████████████████████| 13/13 [00:00<00:00, 119052.30it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.fuse --model microsoft/Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99919f42-88e2-422f-b811-d16ea264bc1d",
   "metadata": {},
   "source": [
    "### Create GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e7666bd-a99f-4488-ad3c-40af50b9fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: lora_fused_model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 32000\n",
      "INFO:gguf.vocab:Setting special token type unk to 0\n",
      "INFO:gguf.vocab:Setting special token type pad to 32000\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {3072, 32064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> F16, shape = {3072, 32064}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:phi-3-mini-ft.gguf: n_tensors = 195, total_size = 7.6G\n",
      "Writing: 100%|███████████████████████████| 7.64G/7.64G [00:17<00:00, 434Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to phi-3-mini-ft.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py './lora_fused_model' --outfile phi-3-mini-ft.gguf --outtype f16 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b16c1-547a-4132-82f4-1abe962f9399",
   "metadata": {},
   "source": [
    "## The GGUF file can now be published on HuggingFace Hub or used in libraries like Llama.CPP. \n",
    "\n",
    "# Happy fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6f6be-287a-4072-8afc-2d8db840ef97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
